{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the PyTorch quickstart tutorial ##\n",
    "\n",
    "#### https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html ####\n",
    "\n",
    "#### Sections:\n",
    "- [Working with Data](#working-with-data)\n",
    "- [Creating Models](#creating)\n",
    "- [Optimizing the Model Parameters](#optimizing)\n",
    "- [Saving and Loading Models](#saving-and-loading-models)\n",
    "####\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"working-data\"></a>\n",
    "### Working with Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch primitives: torch.utils.data.Dataset stores samples and corresponding labels.\n",
    "#torch.utils.data.DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets #package consists of popular datasets, model architectures, and common image transformations for computer vision.\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using FashionMNIST dataset\n",
    "#Dataset includes two arguments: transform and target_transform to modify the samples and labels respectively.\n",
    "#Download training and test data from open datasets.\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "#Pass the dataset to a DataLoader: iterable, automatic batching, sampling, shuffling and multiprocess data loading.\n",
    "\n",
    "#Create data loaders\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "#Iterate through the DataLoader\n",
    "#The FashionMNIST dataset consists of 60,000 training samples and 10,000 testing samples.\n",
    "for X, y in test_dataloader:\n",
    "    #features and labels\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"creating\"></a>\n",
    "### Creating Models ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Create a class that inherits from nn.Module.\n",
    "#Define the layers of the model in the __init__ method.\n",
    "#Specify how data flows through the network in the forward method.\n",
    "\n",
    "#Accelerate using accelerator, if possible.\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "#Define the model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "#Instantiate the model\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"optimizing\"></a>\n",
    "### Optimizing the Model Parameters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define loss function and optimizer\n",
    "#Loss function: measures how well the model performed\n",
    "#Optimizer: updates the model parameters based on the loss function\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "#Training loop\n",
    "#Iterate through the dataset multiple times\n",
    "#For each iteration, perform the following steps:\n",
    "#1. Forward pass: Compute predicted outputs by passing inputs to the model.\n",
    "#2. Compute loss: Calculate the loss using the predicted and true outputs.\n",
    "#3. Backward pass: Compute gradients of the loss with respect to model parameters.\n",
    "#4. Update weights: Adjust model parameters based on gradients.\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(f\"size: {size}\")\n",
    "    model.train() #set the model to training mode\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        #Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        #Backpropagation\n",
    "        loss.backward() #compute gradients of the loss with respect to model parameters\n",
    "        optimizer.step() #update model parameters based on gradients\n",
    "        optimizer.zero_grad() #zero the gradients before the backward pass\n",
    "        \n",
    "        #Print loss every 100 batches\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size= len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() #set the model to evaluation mode\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad(): #no gradients are computed\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device) #move data to the device\n",
    "            pred = model(X) #forward pass\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "size: 60000\n",
      "loss: 2.289942 [    0/60000]\n",
      "loss: 2.291219 [ 6400/60000]\n",
      "loss: 2.276417 [12800/60000]\n",
      "loss: 2.279450 [19200/60000]\n",
      "loss: 2.253651 [25600/60000]\n",
      "loss: 2.226903 [32000/60000]\n",
      "loss: 2.225408 [38400/60000]\n",
      "loss: 2.188240 [44800/60000]\n",
      "loss: 2.180995 [51200/60000]\n",
      "loss: 2.162608 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 2.153136 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "size: 60000\n",
      "loss: 2.148113 [    0/60000]\n",
      "loss: 2.143168 [ 6400/60000]\n",
      "loss: 2.091084 [12800/60000]\n",
      "loss: 2.110883 [19200/60000]\n",
      "loss: 2.044011 [25600/60000]\n",
      "loss: 1.994117 [32000/60000]\n",
      "loss: 2.016250 [38400/60000]\n",
      "loss: 1.933272 [44800/60000]\n",
      "loss: 1.932219 [51200/60000]\n",
      "loss: 1.874278 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.867145 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "size: 60000\n",
      "loss: 1.891934 [    0/60000]\n",
      "loss: 1.861511 [ 6400/60000]\n",
      "loss: 1.749193 [12800/60000]\n",
      "loss: 1.789434 [19200/60000]\n",
      "loss: 1.677787 [25600/60000]\n",
      "loss: 1.636984 [32000/60000]\n",
      "loss: 1.661457 [38400/60000]\n",
      "loss: 1.563379 [44800/60000]\n",
      "loss: 1.585139 [51200/60000]\n",
      "loss: 1.488794 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.505614 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "size: 60000\n",
      "loss: 1.568497 [    0/60000]\n",
      "loss: 1.533432 [ 6400/60000]\n",
      "loss: 1.389989 [12800/60000]\n",
      "loss: 1.457169 [19200/60000]\n",
      "loss: 1.345833 [25600/60000]\n",
      "loss: 1.341298 [32000/60000]\n",
      "loss: 1.356838 [38400/60000]\n",
      "loss: 1.286198 [44800/60000]\n",
      "loss: 1.315045 [51200/60000]\n",
      "loss: 1.220004 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.249383 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "size: 60000\n",
      "loss: 1.323368 [    0/60000]\n",
      "loss: 1.305502 [ 6400/60000]\n",
      "loss: 1.144765 [12800/60000]\n",
      "loss: 1.243395 [19200/60000]\n",
      "loss: 1.124226 [25600/60000]\n",
      "loss: 1.148630 [32000/60000]\n",
      "loss: 1.167773 [38400/60000]\n",
      "loss: 1.113139 [44800/60000]\n",
      "loss: 1.143281 [51200/60000]\n",
      "loss: 1.063691 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.088572 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#The training process is conducted over several iterations (epochs).\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"saving-loading\"></a>\n",
    "### Saving and Loading Models ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A common way to save a model is to serialize the internal state dictionary (containing the model parameters).\n",
    "# The process for loading a model includes re-creating the model structure and loading the state dictionary into it.\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\") #save the model\n",
    "print(\"Saved PyTorch Model State to model.pth\")\n",
    "\n",
    "#Load the model\n",
    "model = NeuralNetwork().to(device) #recreate the model structure\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))  #load the state dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Ankle boot, Actual: Ankle boot\n"
     ]
    }
   ],
   "source": [
    "#The model can be used to make predictions on new data.\n",
    "\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval() #set the model to evaluation mode\n",
    "x, y = test_data[0][0], test_data[0][1] #get the first sample\n",
    "with torch.no_grad():\n",
    "    x = x.to(device) #move data to the device\n",
    "    pred = model(x) #forward pass\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f\"Predicted: {predicted}, Actual: {actual}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
