{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code based on PyTorch Tensors example ##\n",
    "### https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.nn provides all the building blocks for neural networks.\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([3], device='mps:0') and predicted probability: tensor([[0.0960, 0.0985, 0.0932, 0.1142, 0.0906, 0.1050, 0.1123, 0.1050, 0.0923,\n",
      "         0.0928]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#To use the model, we pass it the input data. Do not call model.forward() directly!\n",
    "#Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output. We get the prediction probabilities by passing it through an instance of the nn.Softmax module.\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred} and predicted probability: {pred_probab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#sample minibatch of 3 images of size 28x28 \n",
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "#We initialize the nn.Flatten layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained).\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "#The linear layer is a module that applies a linear transformation on the input using its stored weights and biases.\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.0418, -0.0082,  0.0650, -0.5697, -0.1265, -0.2065,  0.0505,  0.5342,\n",
      "          0.3158,  0.0881,  0.0737, -0.1546, -0.0927,  0.3216, -0.6124,  0.3702,\n",
      "         -0.0636, -0.5036, -0.1843,  0.3864],\n",
      "        [-0.0387, -0.1153,  0.3258, -0.1798, -0.1718, -0.1410,  0.1940,  0.2277,\n",
      "          0.3622, -0.3709, -0.3134,  0.2024,  0.0814,  0.0763, -0.6555,  0.3418,\n",
      "          0.0539, -0.6445, -0.3449,  0.0978],\n",
      "        [ 0.1053, -0.2305,  0.1497, -0.0612, -0.2262, -0.0476,  0.0914,  0.4699,\n",
      "         -0.0464,  0.1042, -0.1497, -0.1919,  0.5446,  0.2746, -0.5642,  0.6542,\n",
      "         -0.2420, -0.5799, -0.1397,  0.4631]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0418, 0.0000, 0.0650, 0.0000, 0.0000, 0.0000, 0.0505, 0.5342, 0.3158,\n",
      "         0.0881, 0.0737, 0.0000, 0.0000, 0.3216, 0.0000, 0.3702, 0.0000, 0.0000,\n",
      "         0.0000, 0.3864],\n",
      "        [0.0000, 0.0000, 0.3258, 0.0000, 0.0000, 0.0000, 0.1940, 0.2277, 0.3622,\n",
      "         0.0000, 0.0000, 0.2024, 0.0814, 0.0763, 0.0000, 0.3418, 0.0539, 0.0000,\n",
      "         0.0000, 0.0978],\n",
      "        [0.1053, 0.0000, 0.1497, 0.0000, 0.0000, 0.0000, 0.0914, 0.4699, 0.0000,\n",
      "         0.1042, 0.0000, 0.0000, 0.5446, 0.2746, 0.0000, 0.6542, 0.0000, 0.0000,\n",
      "         0.0000, 0.4631]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n",
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.Sequential is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like seq_modules.\n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The last linear layer of the neural network returns logits - raw values in [-infty, infty] - which are passed to the nn.Softmax module. The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class. dim parameter indicates the dimension along which the values must sum to 1.\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 1.2378e-02,  2.6317e-02, -2.3282e-02, -1.3513e-02, -1.7735e-02,\n",
      "         -2.6823e-02, -4.2823e-03, -3.0483e-02,  8.5884e-03,  2.8110e-02,\n",
      "          3.1940e-02, -2.2249e-02,  2.9715e-02, -2.6863e-04,  2.8818e-02,\n",
      "          1.2779e-02,  5.3227e-03, -3.1837e-02, -1.5216e-02, -3.1225e-04,\n",
      "         -1.8643e-02,  9.9965e-03, -8.5646e-03, -1.4991e-02,  1.3320e-02,\n",
      "          3.5144e-02,  2.8760e-02, -2.4386e-02, -5.5593e-04, -3.1398e-02,\n",
      "         -7.8283e-03, -1.7105e-03,  9.2155e-03, -6.5503e-03,  1.4591e-02,\n",
      "          2.1194e-02, -3.3191e-02, -2.0388e-03,  5.3728e-04,  7.5538e-03,\n",
      "          3.0849e-02,  6.3860e-03,  3.5062e-02,  2.4802e-02,  2.5923e-02,\n",
      "         -8.1485e-03, -5.0139e-03,  2.8038e-02, -3.4973e-02, -2.1262e-02,\n",
      "          1.3740e-02,  9.6049e-03,  7.1052e-03, -6.2800e-03,  2.0518e-02,\n",
      "         -2.2764e-02, -2.8515e-02,  3.1753e-02, -4.9056e-03, -1.1968e-02,\n",
      "          2.2031e-02,  1.8758e-02,  6.2129e-03, -1.9758e-02,  3.4993e-02,\n",
      "          1.1608e-02,  2.2813e-02, -1.8449e-03,  3.2489e-02,  3.5113e-02,\n",
      "          3.5044e-02, -1.4005e-02, -1.6689e-02, -1.7688e-02, -1.7461e-02,\n",
      "         -2.6874e-02, -1.1200e-02,  1.1901e-02,  2.9184e-02, -1.9074e-02,\n",
      "         -1.0192e-02,  2.4757e-02, -2.9206e-03, -1.8364e-02, -1.2878e-02,\n",
      "         -2.5806e-02,  1.2652e-02, -1.8401e-02,  1.4242e-02, -2.4896e-02,\n",
      "          1.3925e-02,  3.4888e-02, -1.3039e-02,  2.8771e-02, -3.5216e-02,\n",
      "          1.9478e-03,  3.4948e-02, -2.4965e-02, -1.8636e-02, -1.0784e-02,\n",
      "         -1.8001e-02, -3.0547e-02, -1.9024e-02, -2.7321e-02,  3.2484e-02,\n",
      "          8.7957e-04,  2.6067e-02, -1.5578e-02, -1.3017e-02, -3.2377e-02,\n",
      "         -1.7573e-02,  2.7289e-03,  7.9511e-03,  2.4203e-02,  8.6072e-03,\n",
      "          1.0719e-02,  4.5690e-03, -1.3564e-02,  1.1440e-02, -5.1895e-03,\n",
      "          1.7032e-02,  2.3029e-02, -1.9635e-02, -2.3273e-02, -2.0793e-02,\n",
      "         -1.6060e-03,  7.8374e-03,  3.7940e-03,  1.4341e-02,  2.9057e-02,\n",
      "          3.2631e-02,  1.0230e-02,  8.0829e-03,  8.5466e-03, -1.1683e-02,\n",
      "          1.7385e-03,  2.2981e-02,  8.7417e-03, -2.6812e-02, -2.8751e-03,\n",
      "         -9.4447e-03,  1.1428e-02, -2.3334e-02, -1.5745e-02,  2.8550e-02,\n",
      "         -1.1285e-02, -2.6703e-03, -2.4141e-03,  3.5345e-02,  2.0482e-02,\n",
      "         -3.3785e-02,  1.3046e-02, -6.3397e-03,  1.6752e-02, -2.1942e-02,\n",
      "         -2.2284e-02, -2.5503e-02, -3.4649e-02,  9.3624e-04,  2.5165e-02,\n",
      "         -8.2266e-03,  2.7603e-02,  1.8983e-02, -3.3260e-03,  8.3158e-03,\n",
      "          1.4587e-02,  1.6127e-02,  1.8415e-02,  2.4212e-02,  3.4743e-02,\n",
      "          6.7203e-03,  1.9651e-02, -2.6373e-02,  6.7971e-03,  3.5413e-02,\n",
      "         -1.0277e-02,  3.2832e-02,  5.5839e-03,  1.8168e-02,  1.2763e-02,\n",
      "          2.4849e-03,  2.6351e-02, -4.6566e-03,  4.8577e-04,  3.3397e-02,\n",
      "          2.5392e-02,  2.9165e-02,  2.7750e-02, -9.3524e-03, -9.8894e-03,\n",
      "         -2.8724e-02, -1.4706e-02,  2.1964e-02,  5.3337e-03,  4.9356e-04,\n",
      "          1.5240e-02, -2.2106e-03, -2.6196e-02, -3.0111e-02, -1.6035e-02,\n",
      "          1.2908e-04, -2.5897e-02, -1.6529e-02,  1.6025e-03, -7.9510e-03,\n",
      "          1.1362e-02, -3.5083e-02, -3.0170e-02, -2.6560e-02,  3.0051e-02,\n",
      "          1.4094e-03, -2.1453e-02, -2.1537e-02, -1.3594e-02, -5.6261e-04,\n",
      "         -3.9291e-03, -3.5023e-02,  3.1562e-02, -3.0176e-02,  7.7459e-03,\n",
      "          2.3341e-02, -2.9372e-02,  1.4079e-02, -1.1048e-02, -2.5269e-02,\n",
      "         -2.6616e-02,  2.2258e-02,  1.0192e-02, -1.8153e-02, -2.3153e-04,\n",
      "          1.1209e-02,  1.6286e-02,  2.3826e-02, -3.5498e-03, -2.2038e-02,\n",
      "          8.6674e-03,  7.2061e-03, -2.5852e-02,  2.6965e-02,  2.0979e-02,\n",
      "          2.7917e-02,  2.5188e-02, -6.3558e-03,  6.1936e-03, -1.3289e-02,\n",
      "          2.5985e-02, -7.2330e-03,  2.7226e-03,  9.1856e-03, -3.0842e-02,\n",
      "          2.4923e-02,  1.9118e-02, -2.3673e-02, -3.0941e-02, -2.8642e-02,\n",
      "          9.2568e-03,  2.3865e-03, -2.8106e-02,  5.0406e-03, -1.6746e-02,\n",
      "          4.1021e-03,  1.2479e-02,  2.2387e-02, -2.2523e-02,  1.1384e-03,\n",
      "         -3.5281e-02, -3.0473e-02, -3.1287e-02,  5.0388e-03, -2.6783e-02,\n",
      "          1.6608e-05,  1.5935e-02,  2.0873e-02,  2.0346e-02, -1.6225e-02,\n",
      "          3.4923e-02,  2.8006e-02,  2.3146e-02, -2.9348e-02, -4.2599e-03,\n",
      "          2.3507e-03, -4.5308e-03, -1.0247e-02, -1.9554e-02,  3.1568e-02,\n",
      "         -1.7571e-02,  3.2178e-02, -1.5780e-02, -1.0582e-02, -2.9354e-02,\n",
      "         -1.0799e-02, -2.9958e-03,  1.4114e-02, -3.0916e-02,  2.7464e-02,\n",
      "         -3.0408e-02, -2.8097e-02, -1.6405e-02, -1.1998e-02, -3.4421e-02,\n",
      "         -1.0855e-02,  2.1987e-02, -2.5651e-02, -1.7605e-02, -2.6921e-02,\n",
      "          1.5561e-02,  3.1936e-02,  2.3183e-02, -2.2286e-02,  1.7115e-02,\n",
      "         -1.2931e-02,  8.9352e-05, -2.3807e-02, -1.1571e-03, -3.5377e-02,\n",
      "         -1.9793e-02, -2.2730e-02,  7.8629e-03, -3.3661e-02,  8.4824e-03,\n",
      "         -1.4631e-02,  9.7772e-03,  5.4408e-03,  1.5084e-02,  1.1279e-02,\n",
      "          1.9161e-02,  2.6844e-02, -2.5780e-04,  3.0901e-03,  1.6960e-02,\n",
      "          5.6995e-03,  4.2952e-03, -2.9411e-03,  3.1344e-02,  3.5578e-02,\n",
      "          1.0796e-03,  1.7612e-02,  3.5654e-02,  2.7362e-02,  1.8804e-02,\n",
      "         -1.3469e-02, -1.5316e-02,  2.3410e-02, -2.2523e-02, -1.2809e-02,\n",
      "         -2.0979e-02,  1.7439e-02,  2.8458e-02, -1.7624e-02,  3.5013e-02,\n",
      "          2.2322e-02, -9.4058e-03, -1.3682e-02,  8.8478e-03, -1.4312e-03,\n",
      "         -6.4752e-04,  2.6332e-02,  7.5486e-03, -1.4519e-02,  2.1658e-02,\n",
      "          1.3615e-02, -1.7825e-02,  1.8427e-02,  3.4123e-02, -1.1096e-02,\n",
      "         -3.8710e-03, -2.8420e-02, -2.6013e-02, -3.2102e-02, -1.6610e-02,\n",
      "         -9.4345e-03, -1.3018e-02, -2.3472e-02, -3.0574e-02,  1.6444e-02,\n",
      "         -7.6641e-03, -1.9020e-02, -2.2493e-02,  2.3045e-02, -3.4427e-02,\n",
      "         -7.5795e-03,  1.9729e-02, -1.4695e-02,  2.6901e-02, -6.1444e-03,\n",
      "         -1.4016e-02, -2.0908e-02, -3.8095e-03,  4.8072e-03, -9.4877e-03,\n",
      "         -1.4770e-02,  5.8546e-03, -2.8228e-02,  2.6411e-02, -1.5539e-02,\n",
      "         -1.0184e-02,  3.1354e-02,  3.4421e-02,  2.1754e-03,  3.0723e-02,\n",
      "          2.8083e-02, -3.9363e-03,  1.2561e-02,  5.0291e-03,  2.2546e-04,\n",
      "          3.4280e-02,  8.4110e-03,  2.0769e-02, -3.3521e-02, -1.4185e-02,\n",
      "          2.2886e-02,  1.0661e-02, -1.0005e-02,  2.6715e-02, -2.3026e-02,\n",
      "          1.1935e-03, -2.7644e-02,  2.9939e-02, -7.5817e-03, -1.6552e-02,\n",
      "          9.0776e-03,  1.7509e-02,  2.6017e-02,  3.1883e-02,  1.8213e-02,\n",
      "          1.6626e-02,  1.2666e-02, -3.4464e-02, -3.4853e-03,  2.3884e-02,\n",
      "          1.7908e-02, -1.6232e-02, -1.6274e-02, -1.9608e-02,  8.4774e-03,\n",
      "         -2.3557e-02,  2.0568e-03,  5.6674e-03,  6.4185e-03, -1.1470e-02,\n",
      "         -1.1011e-02, -4.4633e-03, -1.1128e-02, -1.4645e-03, -1.1043e-02,\n",
      "          1.0493e-02, -2.5830e-02, -1.4459e-02, -3.1901e-03,  9.4559e-05,\n",
      "          1.3570e-02,  4.5329e-03, -3.5174e-02,  1.6451e-02, -1.9033e-02,\n",
      "         -8.4708e-03, -3.1014e-02,  1.6301e-02, -3.3708e-02, -2.6963e-02,\n",
      "         -1.6193e-02,  2.7414e-02,  7.6129e-03,  2.5993e-02, -2.4490e-04,\n",
      "         -1.7290e-02,  1.0967e-02, -3.2947e-02,  3.3234e-02, -1.2887e-02,\n",
      "          1.5038e-02, -3.0630e-02,  2.5327e-03,  3.0852e-02, -1.8430e-02,\n",
      "         -2.7469e-02,  1.6841e-02,  2.0482e-03,  3.0506e-02, -3.0096e-02,\n",
      "         -1.0111e-02,  1.0632e-02,  3.1660e-03, -2.2894e-03,  2.2388e-02,\n",
      "          1.4130e-02,  6.4595e-03, -2.3904e-02,  6.7836e-03,  2.1012e-02,\n",
      "          9.2002e-03, -1.2083e-02,  2.4760e-02, -3.2806e-03, -2.3289e-02,\n",
      "         -1.2543e-02,  6.3544e-03, -2.8065e-03, -2.1767e-02, -2.3753e-02,\n",
      "          3.5389e-02, -1.2393e-02, -2.8725e-02, -7.8515e-03,  2.4934e-02,\n",
      "         -2.5211e-02,  1.1009e-02,  7.8164e-03,  1.2565e-02, -2.9200e-02,\n",
      "          9.0637e-03,  3.5259e-02,  2.8456e-02, -1.1896e-02, -2.4923e-02,\n",
      "          1.2810e-02, -2.5581e-02, -4.6422e-03, -6.0332e-03, -3.5366e-02,\n",
      "          2.9744e-02,  1.7327e-04, -7.4218e-03, -3.0530e-03, -2.4070e-02,\n",
      "          2.6366e-02, -2.5568e-02, -1.8473e-02,  2.1387e-02, -3.2419e-03,\n",
      "          1.0005e-02, -7.8107e-03,  8.2365e-03, -3.3871e-02, -2.6937e-02,\n",
      "          6.6351e-03,  2.5903e-02,  4.7482e-03,  8.8597e-03,  3.3184e-02,\n",
      "         -1.0193e-02,  3.2774e-02, -8.3154e-03,  2.9783e-02,  8.8114e-03,\n",
      "         -1.0510e-02, -2.8912e-02, -3.3308e-02,  7.0687e-03,  3.1257e-02,\n",
      "         -2.3216e-02, -1.0387e-02,  2.8788e-02,  2.2286e-02, -3.4294e-02,\n",
      "         -5.4946e-03,  3.2648e-02,  6.9456e-03,  1.4522e-03, -2.8654e-02,\n",
      "          3.3224e-02, -3.1031e-02,  2.3613e-02,  1.1296e-02,  8.1474e-03,\n",
      "         -2.4537e-02,  2.1243e-02, -2.0650e-02,  2.0817e-02,  3.1430e-02,\n",
      "         -1.6807e-03, -2.9201e-02, -2.3614e-02, -1.0430e-02,  1.5120e-02,\n",
      "          1.8224e-02,  2.2414e-02,  3.5057e-02,  1.3821e-02,  1.2193e-02,\n",
      "          5.3453e-03,  3.1029e-02, -1.1387e-02,  1.4171e-02, -2.8596e-02,\n",
      "         -1.9171e-02,  3.3107e-02,  1.9834e-02, -1.3101e-02, -3.5052e-02,\n",
      "          1.6798e-02,  2.0828e-02,  1.8179e-02, -4.2019e-03,  2.8057e-02,\n",
      "         -2.5947e-02,  3.0236e-03,  2.8610e-02, -1.0929e-02, -1.4606e-02,\n",
      "         -1.2148e-02,  1.8075e-02,  1.1392e-02,  2.1246e-02,  3.0196e-02,\n",
      "          2.4352e-02, -9.7159e-03, -6.1786e-03,  1.7671e-02,  2.8649e-02,\n",
      "         -3.2744e-02, -2.7701e-02,  2.8978e-02, -3.5589e-02,  2.4808e-02,\n",
      "         -2.4617e-02, -7.6708e-04, -1.7610e-02,  1.6341e-02,  1.2484e-02,\n",
      "         -3.1767e-03,  2.0887e-02,  2.3635e-02,  1.4065e-02,  3.2771e-02,\n",
      "         -1.9942e-02, -1.6700e-02,  8.2240e-03,  2.6683e-02,  1.7631e-02,\n",
      "          1.2342e-02, -5.4302e-03, -1.0631e-02, -2.7122e-02, -8.3605e-03,\n",
      "          6.8025e-03, -3.1328e-02,  3.5346e-02, -9.9771e-03, -1.4874e-02,\n",
      "         -6.7126e-03, -2.6917e-02,  1.3915e-02,  5.8738e-03, -2.4490e-02,\n",
      "         -2.5909e-02,  2.8586e-02,  2.3869e-02, -2.8790e-02,  1.3446e-02,\n",
      "         -1.5024e-02, -1.3357e-02, -6.6213e-03, -1.0926e-02,  1.9275e-02,\n",
      "          4.5154e-03, -3.4295e-02,  7.8388e-03,  1.6089e-02,  1.0799e-02,\n",
      "         -2.0255e-02, -3.4467e-02, -1.5059e-03, -1.5512e-02,  3.1975e-02,\n",
      "         -2.3804e-02,  9.5991e-03, -2.7926e-02, -1.1328e-02, -2.9985e-02,\n",
      "         -2.8607e-02, -2.0090e-02, -2.1463e-02, -1.2289e-02, -8.1321e-03,\n",
      "         -6.9269e-04,  1.3092e-02, -1.6796e-03,  6.1627e-03,  9.0512e-04,\n",
      "         -2.0112e-02,  2.6782e-02, -1.5538e-03, -1.3058e-02, -2.2427e-02,\n",
      "          2.4505e-02, -1.3508e-02,  3.1492e-02, -2.9420e-02, -1.7196e-02,\n",
      "         -4.6800e-03,  3.1161e-02,  3.5291e-02,  4.5227e-03, -2.3232e-02,\n",
      "         -1.4567e-02,  2.0648e-02, -1.1454e-03,  8.4536e-03,  2.6106e-02,\n",
      "         -1.8983e-02,  2.3918e-02, -2.6751e-02,  3.2113e-02,  1.0087e-02,\n",
      "         -2.1363e-02, -2.1564e-02,  2.4782e-02, -2.8947e-02,  2.1771e-02,\n",
      "         -3.3263e-02,  2.6046e-02,  3.5269e-02, -1.6047e-02, -1.2398e-02,\n",
      "          6.9247e-03,  3.9179e-03, -1.8183e-02, -2.8928e-03, -2.9105e-02,\n",
      "          2.5803e-04,  7.5416e-03,  1.9851e-02, -1.5473e-02,  1.5014e-02,\n",
      "          2.9459e-02, -2.9338e-02, -9.7220e-03,  2.5240e-02,  2.2632e-02,\n",
      "          1.6450e-02,  1.9302e-02,  2.1900e-02, -1.2384e-02,  1.3250e-02,\n",
      "         -8.5168e-03,  4.6564e-03, -8.1759e-03, -3.4742e-03, -3.0768e-02,\n",
      "          1.8332e-02, -2.3506e-02,  1.6923e-02, -4.0966e-03, -2.7308e-02,\n",
      "         -3.1712e-02, -1.9495e-02,  2.5841e-02,  3.4980e-03,  5.4890e-03,\n",
      "          1.4229e-02, -7.4304e-03,  2.1191e-02,  1.2694e-02,  4.3937e-03,\n",
      "          3.0582e-02,  3.8552e-04,  2.3383e-03, -1.6067e-02, -2.3717e-02,\n",
      "         -8.9385e-03,  7.6918e-03, -5.3795e-03,  2.1203e-02,  3.0644e-02,\n",
      "          2.9502e-03,  2.2359e-03,  1.1986e-02, -4.1912e-03, -2.3688e-02,\n",
      "         -1.2376e-02, -2.1891e-02,  3.7831e-03, -3.4346e-02, -2.5768e-02,\n",
      "          2.7654e-02,  6.5774e-03, -2.5999e-02, -1.2128e-02, -1.1451e-02,\n",
      "         -2.7300e-02, -3.3064e-02,  2.2437e-02,  3.4910e-02]], device='mps:0',\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0057], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 1.5538e-02,  1.4408e-03,  1.1937e-02,  4.3143e-02, -3.6393e-02,\n",
      "          1.4546e-02, -1.6398e-02,  4.1123e-02,  1.1826e-02,  2.0330e-02,\n",
      "          3.5328e-03,  3.4678e-02,  3.4273e-02, -4.0025e-02, -3.6660e-02,\n",
      "          3.8370e-02, -4.4425e-03,  3.5058e-02, -4.3415e-02, -3.3956e-02,\n",
      "         -2.3681e-02, -4.1532e-02, -9.6559e-03,  1.9896e-02, -8.0588e-03,\n",
      "          3.5120e-02, -2.8989e-03, -3.9119e-03, -4.5664e-03,  4.2938e-02,\n",
      "         -2.1071e-02,  1.6614e-03,  1.1851e-02, -3.3299e-02,  2.5135e-02,\n",
      "          3.1332e-02, -1.9372e-02, -1.1555e-02,  3.3446e-02,  3.7994e-02,\n",
      "          1.2922e-02,  6.0298e-03,  2.3062e-02,  1.7999e-02,  3.6822e-02,\n",
      "         -2.3795e-02,  3.5414e-02, -2.9487e-02, -1.9280e-03, -1.9705e-02,\n",
      "          2.5286e-03,  1.7374e-02,  7.6038e-03, -3.7798e-02,  1.5676e-02,\n",
      "         -1.9342e-02, -3.1895e-02,  1.6093e-02,  4.2150e-03,  5.2230e-04,\n",
      "          2.6672e-02, -1.9468e-02, -3.4264e-02, -1.3534e-02, -2.4488e-02,\n",
      "          6.5622e-03,  2.4219e-03, -9.9799e-03,  4.1286e-02, -4.3902e-02,\n",
      "          3.5797e-02,  2.6627e-02, -3.4931e-02,  1.6209e-02,  4.0169e-02,\n",
      "         -6.3223e-03,  1.4036e-02,  9.9152e-03, -2.6062e-04, -7.8626e-04,\n",
      "         -3.5353e-02,  3.3807e-02, -2.8150e-02, -2.2120e-02, -7.3128e-03,\n",
      "         -3.0995e-03, -1.3506e-02,  6.6299e-04, -3.0692e-02, -3.3889e-02,\n",
      "         -2.7139e-02, -3.2624e-02, -1.2481e-03,  4.3848e-02,  5.5144e-05,\n",
      "          3.0024e-02, -3.9989e-02, -3.8129e-02, -1.7049e-02, -2.7595e-02,\n",
      "          2.7679e-02, -4.3088e-02, -2.7906e-02,  3.7086e-02,  7.1466e-03,\n",
      "          3.8007e-02,  1.9901e-02, -3.7938e-02,  3.9199e-02,  9.3121e-03,\n",
      "          1.7715e-02,  9.5042e-03, -3.4951e-03, -7.5073e-03, -3.1575e-02,\n",
      "         -3.3957e-03, -1.2748e-02,  1.4724e-02,  3.5902e-02,  2.9903e-02,\n",
      "         -3.9485e-02,  1.2469e-02, -2.6139e-02,  2.1046e-02, -2.1660e-02,\n",
      "          8.1314e-03,  2.6986e-02, -9.3944e-04,  4.3862e-02, -2.0683e-03,\n",
      "         -3.2932e-02, -8.1064e-05, -2.2033e-02, -3.0228e-02, -1.3004e-02,\n",
      "         -4.2310e-02,  1.5227e-02, -1.7119e-03,  3.8193e-02,  3.0290e-03,\n",
      "         -4.1904e-02, -2.6874e-02, -2.1244e-02,  3.7721e-02,  3.7797e-02,\n",
      "          1.6233e-03,  7.9476e-03,  2.8402e-03, -2.6814e-02, -4.1410e-02,\n",
      "         -3.3040e-02,  4.0182e-02,  3.7194e-02, -1.8900e-02,  4.2202e-02,\n",
      "          2.3257e-02, -4.0905e-02,  7.8160e-03,  2.1328e-02, -9.3293e-03,\n",
      "         -1.9092e-02,  9.2698e-04,  4.0502e-02,  9.4347e-03,  2.3988e-02,\n",
      "          2.1544e-02, -7.4757e-03,  3.5176e-02, -2.2063e-02,  3.1859e-02,\n",
      "          2.8676e-02, -3.3704e-02, -4.0658e-02,  3.6516e-02, -3.4051e-02,\n",
      "         -4.3471e-02, -8.1564e-03, -1.2708e-02, -2.5285e-02,  1.0149e-02,\n",
      "         -1.9250e-02, -1.0543e-02,  1.3012e-02, -3.8251e-02,  1.3638e-02,\n",
      "         -2.2749e-02, -1.7637e-02, -6.4653e-03,  3.6043e-02,  1.9295e-02,\n",
      "          1.2491e-02,  2.9026e-02, -3.2473e-02,  1.0746e-03,  2.5475e-02,\n",
      "          1.4132e-02,  2.4441e-02,  5.5881e-04, -2.6101e-02, -3.3757e-02,\n",
      "          1.4617e-02, -1.4021e-02, -3.5832e-02,  4.0351e-02, -2.5927e-02,\n",
      "          1.4809e-02,  2.4506e-02,  3.9632e-02, -3.1316e-02,  6.8502e-03,\n",
      "          3.3841e-02,  2.4100e-04, -4.1650e-02,  3.9432e-02, -1.6725e-02,\n",
      "         -3.0859e-02,  2.7233e-02, -4.2215e-02,  3.7858e-03, -1.1141e-02,\n",
      "          1.5329e-03,  2.7747e-03, -1.7152e-02, -3.2765e-02, -3.9977e-02,\n",
      "         -3.3643e-02,  2.7613e-02, -8.3305e-04, -3.0941e-02,  2.3197e-02,\n",
      "          1.2857e-03,  1.7760e-02, -3.7626e-02,  3.9437e-02, -2.6995e-02,\n",
      "          2.9854e-02,  4.0863e-02, -1.2479e-03, -6.9626e-03, -4.1629e-03,\n",
      "          1.2166e-02, -1.7285e-02,  1.5483e-02,  3.8831e-02,  1.0505e-02,\n",
      "          2.4543e-02,  2.3019e-02,  4.0563e-02,  1.1549e-02,  1.5227e-02,\n",
      "         -4.1776e-02,  1.7022e-02, -2.9767e-03, -2.8240e-03,  3.9212e-02,\n",
      "         -1.4210e-02,  3.2529e-02, -1.3667e-02, -4.2058e-02,  2.3609e-02,\n",
      "         -1.5995e-02,  4.0803e-02,  4.3199e-02, -3.1709e-03,  2.4558e-03,\n",
      "         -1.0955e-03,  4.3795e-02,  3.0190e-02,  3.0307e-02,  3.2697e-02,\n",
      "         -3.2565e-02,  3.5100e-02, -2.9485e-02,  6.2774e-03, -2.1983e-02,\n",
      "          7.8031e-03, -3.0998e-02, -3.2511e-02, -3.4161e-02, -4.0657e-02,\n",
      "          2.1535e-02, -1.2623e-02, -1.4484e-03,  3.9463e-02,  2.0309e-02,\n",
      "         -1.8558e-02, -2.2386e-03,  3.5120e-02, -3.5960e-02,  2.1162e-02,\n",
      "          3.5944e-02,  7.7901e-03, -2.4774e-02,  8.7779e-04, -2.3836e-02,\n",
      "         -3.6397e-02, -1.8390e-03,  2.2560e-02, -9.5052e-03, -2.0775e-02,\n",
      "         -1.8431e-03, -8.3505e-03,  6.5481e-03, -1.6506e-02, -2.9393e-02,\n",
      "         -1.8116e-02,  4.2578e-02, -1.5155e-02,  1.7731e-02,  2.8475e-02,\n",
      "         -2.6508e-02, -1.3478e-02,  1.3637e-02,  3.8892e-02,  2.7430e-02,\n",
      "          2.5088e-02,  2.5634e-02, -1.7590e-02,  3.3087e-02, -1.8049e-02,\n",
      "          3.8897e-02,  1.2778e-02, -3.4266e-02, -8.3680e-04,  3.3271e-02,\n",
      "         -3.0676e-02, -3.9512e-02, -3.7033e-02,  2.9477e-03,  3.0816e-02,\n",
      "         -2.4666e-02, -1.9009e-02, -3.7407e-02, -2.6873e-02, -3.2216e-03,\n",
      "         -3.9184e-02,  1.2284e-02,  4.1061e-03,  1.6674e-02,  4.6854e-03,\n",
      "         -4.1530e-02, -3.2925e-02, -2.3317e-02, -2.1886e-02, -1.0432e-02,\n",
      "          2.2970e-02,  2.9416e-02, -3.5026e-02,  1.7789e-02,  3.9707e-02,\n",
      "         -2.7940e-02, -7.9303e-03, -1.0661e-02,  4.0240e-02,  7.1332e-03,\n",
      "          1.9955e-02,  1.3201e-02,  1.2248e-02,  3.5604e-02,  3.0508e-03,\n",
      "          4.1828e-02,  9.8389e-03, -3.7280e-02, -2.1860e-03,  3.7494e-02,\n",
      "         -3.4289e-02, -1.0841e-02, -3.8171e-02,  2.3293e-02, -1.2448e-02,\n",
      "          3.7354e-02,  2.5245e-03, -3.4107e-02,  1.4107e-02, -2.8637e-03,\n",
      "          5.9118e-03,  2.1894e-02,  7.0133e-03, -1.0577e-02, -4.0988e-02,\n",
      "         -2.6902e-02,  2.0985e-02,  5.7676e-03, -3.6281e-02,  2.2035e-02,\n",
      "          2.5055e-02, -4.1208e-03,  3.1572e-02, -1.0222e-02,  2.0148e-02,\n",
      "          2.7293e-02,  3.5118e-02,  8.4171e-03,  8.3978e-03,  1.5631e-02,\n",
      "         -4.1492e-02,  5.7816e-03, -2.6779e-02, -2.4430e-02, -2.9240e-02,\n",
      "          3.1336e-02,  8.9004e-03, -2.5332e-02,  1.8919e-02,  2.1846e-02,\n",
      "         -3.3177e-02, -3.0127e-02,  1.4523e-02, -2.1906e-02,  2.2250e-02,\n",
      "          2.1507e-02, -2.2027e-02, -2.7011e-02, -1.5880e-03,  3.3247e-02,\n",
      "         -1.3433e-02, -3.7398e-02,  3.3585e-02, -4.1739e-02, -9.3822e-03,\n",
      "          8.2374e-03, -1.4631e-02,  4.1850e-02, -4.3802e-02,  6.4214e-04,\n",
      "         -6.5132e-03,  2.6544e-02, -1.1608e-03, -1.6098e-02, -1.6153e-02,\n",
      "          4.0188e-03, -1.6917e-02,  4.2927e-02, -1.0457e-02,  4.1036e-02,\n",
      "         -1.1667e-02,  4.0391e-02,  2.8061e-02,  1.4379e-02, -4.2837e-02,\n",
      "          2.0703e-02, -3.0394e-02, -2.8463e-02, -1.1922e-02,  2.7325e-02,\n",
      "          1.7498e-02, -4.1175e-02,  2.4914e-02,  2.1562e-02, -3.7003e-02,\n",
      "          1.4294e-02, -3.8525e-02,  1.7081e-02, -4.2686e-02, -1.8465e-02,\n",
      "         -2.1112e-02,  3.0890e-02,  2.0440e-02, -3.4905e-02,  2.7251e-02,\n",
      "          2.6693e-02,  2.3173e-02, -3.7014e-02, -3.7736e-02,  2.1359e-02,\n",
      "         -2.5915e-03,  9.2939e-03,  3.4243e-02, -6.1897e-03,  3.0114e-03,\n",
      "         -4.9408e-03,  4.2210e-02, -2.9245e-02,  4.3366e-02, -1.5262e-02,\n",
      "          2.6172e-02,  4.0881e-03,  2.4019e-02,  3.0171e-02,  3.4534e-02,\n",
      "         -3.9286e-02, -1.3582e-02, -4.4175e-02, -2.2206e-02,  4.1748e-02,\n",
      "         -7.0484e-03, -2.9877e-02,  7.1546e-03,  1.8376e-02, -2.3316e-02,\n",
      "          3.4025e-02,  1.5308e-02,  3.1269e-02,  3.9070e-02, -3.0755e-03,\n",
      "          2.8362e-02,  1.0393e-02, -2.0629e-02,  3.9403e-02, -1.0539e-02,\n",
      "          1.5403e-03, -2.1115e-02, -1.5082e-02,  3.9024e-02,  1.5957e-02,\n",
      "          4.3445e-02,  1.6297e-02,  3.2019e-02,  4.5517e-03,  3.9736e-02,\n",
      "         -1.1124e-02,  3.6663e-02]], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0020], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 2.6468e-02,  5.9278e-03, -3.2290e-02, -3.6275e-02,  1.9359e-02,\n",
      "         -3.4213e-02,  2.6844e-02, -1.1099e-02,  6.5115e-03, -3.5958e-02,\n",
      "          3.4705e-02,  3.1460e-02,  2.5145e-02, -2.8738e-02, -2.4520e-02,\n",
      "         -2.3570e-02,  1.8367e-02, -3.7051e-02,  1.1757e-02,  3.7044e-02,\n",
      "         -1.5125e-02, -6.7894e-04,  1.6667e-02,  2.1349e-02, -3.1199e-02,\n",
      "         -1.8374e-02, -4.9176e-03,  1.4570e-02, -3.6319e-02,  4.3797e-03,\n",
      "          2.8136e-02,  3.4862e-03,  2.3013e-02,  1.5669e-02, -3.8349e-02,\n",
      "          4.5880e-03,  4.0151e-02, -1.8573e-02, -2.0181e-02,  1.4004e-02,\n",
      "          3.7220e-03,  6.0400e-03, -3.5554e-02,  2.6515e-02, -2.8117e-02,\n",
      "          3.7474e-03, -5.1856e-03, -3.3447e-02,  1.2535e-02,  3.1123e-02,\n",
      "         -2.6563e-02,  8.8384e-03,  1.3857e-02,  8.7314e-03, -1.7923e-02,\n",
      "         -1.2795e-03,  3.6286e-02,  3.9453e-02, -2.7554e-02, -2.9696e-02,\n",
      "         -4.0197e-02,  3.9207e-02, -2.1680e-02, -2.0133e-02, -3.7742e-02,\n",
      "         -1.0554e-02,  3.6434e-02,  3.5878e-02, -1.0402e-02,  4.0165e-02,\n",
      "         -1.4584e-02,  2.5486e-02,  4.2347e-02, -1.7708e-02, -3.7606e-02,\n",
      "         -4.1183e-02,  2.5560e-02,  7.6792e-03, -4.0909e-02,  7.1262e-04,\n",
      "          2.7530e-02,  7.8653e-04, -6.9620e-03, -8.1265e-03,  3.0946e-02,\n",
      "          4.7428e-03, -3.4820e-02, -3.5518e-02, -3.3227e-02, -3.6979e-02,\n",
      "          4.4905e-04,  3.2147e-02,  3.1566e-02,  1.0770e-02, -7.1431e-03,\n",
      "          2.8252e-02,  1.1128e-02, -9.7908e-04,  1.5435e-02,  3.9589e-02,\n",
      "         -2.0660e-03,  3.7993e-02,  3.3290e-02,  1.9992e-02,  3.1972e-02,\n",
      "         -2.9754e-02, -1.6026e-02, -1.7587e-02,  9.1806e-03, -1.8110e-02,\n",
      "         -2.2459e-02, -3.7211e-02,  7.8550e-04, -2.8722e-02,  3.5839e-02,\n",
      "         -3.4035e-02,  2.9299e-02, -1.3781e-02,  1.2492e-02,  3.7674e-02,\n",
      "          3.2480e-02,  2.7420e-02,  4.9365e-03,  1.2370e-02, -3.1447e-02,\n",
      "         -5.0269e-03,  3.1188e-02,  3.3054e-03, -3.7764e-02,  3.2982e-02,\n",
      "         -2.3068e-03,  3.2396e-02,  3.9094e-02, -3.7159e-02, -1.3033e-02,\n",
      "          3.0811e-03, -4.1375e-02, -3.1672e-02,  3.3830e-02, -1.1268e-02,\n",
      "          3.6357e-02,  3.5960e-02, -2.5738e-02,  2.8746e-02, -4.7819e-03,\n",
      "         -8.7502e-03, -3.3851e-02, -9.1280e-03, -2.8772e-02,  4.2939e-02,\n",
      "          1.4081e-02, -3.5503e-02,  2.1958e-02, -1.1291e-02, -1.4881e-02,\n",
      "          5.8471e-03, -5.1749e-03,  3.0251e-02, -2.1250e-02, -3.4409e-02,\n",
      "          3.6853e-03,  4.4137e-02,  7.2921e-03, -3.1183e-02, -4.4176e-02,\n",
      "          4.0166e-02, -2.3245e-02, -2.3533e-02,  2.3643e-02, -3.3743e-02,\n",
      "         -1.3500e-02,  3.6311e-02, -3.0045e-02,  2.5526e-02,  5.6457e-03,\n",
      "          3.3091e-02, -2.3127e-03, -2.7002e-03,  5.2779e-03,  1.0707e-03,\n",
      "         -3.9104e-02,  6.5500e-03,  2.8674e-02,  1.2918e-02,  3.4254e-02,\n",
      "         -3.8349e-02, -2.2109e-02,  1.2258e-02, -9.9096e-03,  1.6892e-02,\n",
      "          4.2592e-02,  2.4373e-02,  3.0911e-02, -4.7446e-03,  2.0157e-02,\n",
      "         -3.6502e-02, -3.4023e-02,  1.2412e-02,  4.1310e-02, -4.2283e-02,\n",
      "          3.4791e-02,  2.1430e-02, -7.1652e-03,  4.3976e-02,  3.9132e-02,\n",
      "          2.2384e-02, -2.6737e-02, -1.8923e-03,  4.1421e-02, -4.3969e-02,\n",
      "         -5.8216e-03, -3.3424e-02, -1.4091e-02, -4.3001e-02, -1.3151e-02,\n",
      "          3.7596e-02,  3.4875e-03,  3.1423e-03, -2.4654e-03, -5.5442e-03,\n",
      "         -2.9263e-02, -8.0443e-03,  4.5031e-03, -2.7740e-03,  7.4877e-03,\n",
      "         -3.1921e-02, -1.0478e-02, -2.4980e-02, -1.6548e-02, -1.9930e-02,\n",
      "         -4.0997e-02, -1.9922e-02,  1.1519e-02,  4.1661e-02,  2.7919e-02,\n",
      "          3.5604e-02, -6.7501e-03, -2.4653e-03,  3.7495e-02,  2.7629e-02,\n",
      "          3.9195e-02, -1.7641e-03, -1.8281e-02, -1.4857e-02,  2.0944e-02,\n",
      "         -1.4044e-02, -7.2683e-03, -1.6648e-02,  2.6141e-02, -4.3114e-02,\n",
      "         -1.3580e-03,  3.0552e-03, -2.4450e-02, -3.2000e-02, -2.2572e-02,\n",
      "          1.4840e-02,  2.9419e-02, -1.7175e-02, -4.4575e-03,  4.0712e-02,\n",
      "         -3.7228e-03, -7.7277e-03, -3.9777e-02, -2.9532e-02,  1.9381e-02,\n",
      "         -1.7109e-02, -2.0467e-04, -1.4140e-04,  1.7682e-02, -4.2422e-02,\n",
      "         -2.9672e-03,  3.3592e-02,  7.5328e-03, -4.1578e-02,  3.5983e-04,\n",
      "         -2.8753e-02, -3.7782e-02, -1.2646e-02, -2.4134e-02,  2.5077e-02,\n",
      "         -3.9807e-02, -2.3504e-02, -1.1699e-02, -2.6393e-03,  7.4765e-03,\n",
      "          6.9787e-03, -1.4355e-02,  1.9607e-02, -2.5490e-02,  4.2937e-02,\n",
      "          1.0033e-02, -1.7594e-02, -2.4536e-02,  3.8718e-02, -3.7355e-02,\n",
      "         -3.8245e-02, -2.3191e-02,  3.8497e-02,  1.9646e-02, -4.7707e-03,\n",
      "         -2.5520e-02,  1.2803e-02,  2.5809e-02, -2.1893e-02,  7.2040e-03,\n",
      "         -2.5948e-02,  4.5653e-03, -2.2322e-02,  2.0020e-02,  1.7358e-03,\n",
      "          2.8419e-02,  1.2874e-02, -2.0301e-02,  4.1107e-02, -1.0744e-03,\n",
      "          3.1816e-02, -2.0329e-02, -7.6604e-04, -5.9858e-03, -3.5817e-02,\n",
      "         -1.7523e-02,  3.8514e-02, -8.2284e-03,  2.7784e-03,  1.3664e-02,\n",
      "          1.5162e-02,  1.9788e-02, -8.4103e-03,  1.2188e-02, -2.5195e-02,\n",
      "          1.8884e-02,  2.8425e-02,  3.7397e-02,  2.8979e-02,  2.4097e-02,\n",
      "         -2.7916e-03,  5.2371e-03,  3.4918e-02, -7.6450e-04,  3.0994e-03,\n",
      "          6.2268e-03,  3.2305e-02,  1.9051e-02,  3.1736e-02,  1.7694e-02,\n",
      "          2.7691e-02,  2.9063e-02, -3.4701e-04,  2.5880e-02, -1.2269e-02,\n",
      "         -1.2285e-02,  1.3792e-02, -2.2066e-02, -3.7056e-02,  5.5787e-05,\n",
      "          1.6193e-02,  1.5001e-02, -9.3301e-03,  3.7890e-02,  6.8099e-03,\n",
      "          4.0609e-02, -8.1666e-03, -2.9518e-02,  3.0653e-02,  1.8916e-02,\n",
      "          2.1014e-02, -2.2417e-02,  2.7562e-02,  3.3177e-02, -1.8732e-02,\n",
      "         -3.3817e-02,  9.9488e-03,  1.4858e-02, -5.7088e-03,  3.8075e-02,\n",
      "          3.2509e-02,  1.7478e-02, -9.7735e-03,  1.1165e-03, -7.8182e-03,\n",
      "          4.2156e-02, -3.0812e-02, -2.5293e-02,  2.7249e-02, -3.6422e-02,\n",
      "         -1.2788e-02, -2.2977e-02,  1.5779e-03,  1.1222e-02, -1.1634e-02,\n",
      "         -7.0092e-04,  1.4679e-02,  1.1535e-02, -7.6110e-03,  1.7894e-02,\n",
      "          3.8276e-02, -4.4116e-02, -9.2375e-03, -3.5737e-03,  2.7539e-02,\n",
      "         -2.4903e-02, -1.4759e-02, -1.7881e-02,  3.1065e-03, -4.1103e-02,\n",
      "          5.4776e-03, -2.3160e-02,  3.7594e-02,  3.4060e-02, -3.6656e-02,\n",
      "          2.7909e-02, -7.8617e-03, -5.8228e-03, -1.1369e-02,  2.9251e-02,\n",
      "         -1.9576e-02, -4.4048e-02, -1.9964e-03,  3.5075e-02, -2.8964e-02,\n",
      "         -3.0282e-02, -4.3013e-02, -4.6525e-05,  2.8517e-02,  6.7599e-03,\n",
      "         -5.6402e-04,  4.2632e-02,  1.1704e-03,  1.5912e-02, -1.2505e-03,\n",
      "          2.4480e-02,  3.4526e-02, -4.0216e-02,  1.2262e-02, -3.9612e-03,\n",
      "         -1.1414e-02,  3.4207e-02,  1.8992e-02,  3.3649e-02,  1.2328e-02,\n",
      "          7.4173e-03,  2.7141e-02,  5.7200e-03,  3.8075e-02, -3.8492e-02,\n",
      "         -1.4930e-02, -2.3212e-02, -4.3437e-02, -3.4090e-02, -2.9205e-02,\n",
      "         -2.8438e-02,  2.9929e-02, -3.1000e-02,  1.8803e-02,  2.3295e-04,\n",
      "          1.5885e-02,  4.0570e-03,  2.4522e-02,  3.3446e-02,  1.6038e-02,\n",
      "         -2.1830e-02, -6.1350e-03, -1.0558e-02, -1.6942e-02, -2.9408e-02,\n",
      "         -2.4170e-02, -1.6844e-02,  1.2392e-02,  2.7935e-02,  1.0207e-02,\n",
      "          3.2882e-02,  6.1662e-03, -3.1013e-02,  8.3725e-03, -4.3722e-02,\n",
      "          6.5207e-04,  1.1762e-02,  1.3157e-02,  2.5895e-02, -4.0231e-02,\n",
      "          2.4052e-02,  4.2839e-02,  1.5698e-02, -2.2180e-02, -3.9528e-02,\n",
      "         -1.2673e-02, -2.3399e-02, -3.4376e-02, -4.8824e-04, -3.9448e-02,\n",
      "          1.3618e-02,  1.2527e-02,  1.0749e-02,  2.6947e-02,  2.8175e-02,\n",
      "         -2.5481e-02,  3.9239e-02,  2.5795e-02,  1.5065e-02, -1.0903e-02,\n",
      "          6.7825e-03,  3.4815e-02, -1.0385e-02, -1.8514e-02,  2.7142e-02,\n",
      "         -2.1620e-02,  6.3628e-03,  4.1981e-02,  8.5707e-03, -1.0957e-02,\n",
      "         -1.1141e-02,  3.0381e-02]], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0230], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Many layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. Subclassing nn.Module automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s parameters() or named_parameters() methods.\n",
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
